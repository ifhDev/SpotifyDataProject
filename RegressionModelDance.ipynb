{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from spotify_cleaner import clean_data, remove_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataset.csv', index_col=0)\n",
    "df = clean_data(df)\n",
    "df = remove_duplicates(df)\n",
    "\n",
    "# reclassifying sleep genre as \"negative control group\"\n",
    "df.loc[df['track_genre'] == 'sleep', 'danceability'] = 0\n",
    "\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare modelling data (feature & target definition, train/test-split)\n",
    "features_cols = ['energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'valence', 'tempo']\n",
    "target_col = ['danceability']\n",
    "\n",
    "features = df[features_cols]\n",
    "target = df[target_col]\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features\n",
    "scaler = StandardScaler()\n",
    "features_train_scaled = scaler.fit_transform(features_train)\n",
    "features_test_scaled = scaler.transform(features_test)\n",
    "\n",
    "# polynomial features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "features_train_poly = poly.fit_transform(features_train_scaled)\n",
    "features_test_poly = poly.transform(features_test_scaled)\n",
    "\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(features_train_poly, target_train)\n",
    "\n",
    "# predict & evaluate\n",
    "target_pred_poly = model_poly.predict(features_test_poly)\n",
    "\n",
    "rmse_poly = np.sqrt(mean_squared_error(target_test, target_pred_poly))\n",
    "r2_poly = r2_score(target_test, target_pred_poly)\n",
    "\n",
    "print('RMSE: {:.4f}'.format(rmse_poly))\n",
    "print('R² Score: {:.4f}'.format(r2_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise actual vs predicted danceability\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=target_test.values.flatten(), y=target_pred_poly.flatten(), alpha=0.25)\n",
    "plt.plot([0, 1], [0, 1], '--', color='red')\n",
    "plt.xlabel('Actual Danceability')\n",
    "plt.ylabel('Predicted Danceability')\n",
    "plt.title('Actual vs. Predicted Danceability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verdict\n",
    "Better than pure linear regression, but still not good. Best scores: RMSE: 0.1408, R² Score: 0.3999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise RF regression model\n",
    "rf_model = RandomForestRegressor(random_state=42, min_samples_split=5, min_samples_leaf=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "# search_space = {\n",
    "#     'n_estimators': np.geomspace(250, 400, num=5, dtype='int'),\n",
    "#     'max_depth': np.geomspace(18, 25, num=5, dtype='int')\n",
    "#     }\n",
    "\n",
    "# grid_search = GridSearchCV(estimator=rf_model, param_grid=search_space, cv=3, scoring='r2', n_jobs=-1)\n",
    "# grid_search.fit(features_train, target_train.values.ravel()) #flattens df!\n",
    "\n",
    "# print('Best parameters:', grid_search.best_params_)\n",
    "\n",
    "# note: Best parameters: {'max_depth': np.int64(25), 'n_estimators': np.int64(400)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction and evaluation (originally \"best estimator\")\n",
    "best_rf_model = RandomForestRegressor(random_state=42, max_depth=25, n_estimators=400, min_samples_split=5, min_samples_leaf=2)\n",
    "target_pred_rf = best_rf_model.predict(features_test)\n",
    "\n",
    "rmse_rf = np.sqrt(mean_squared_error(target_test, target_pred_rf))\n",
    "r2_rf = r2_score(target_test, target_pred_rf)\n",
    "\n",
    "print('Optimised Random Forest RMSE: {:.4f}'.format(rmse_rf))\n",
    "print('Optimised Random Forest R² Score: {:.4f}'.format(r2_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise actual vs predicted danceability\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=target_test.values.flatten(), y=target_pred_rf.flatten(), alpha=0.25)\n",
    "plt.plot([0, 1], [0, 1], '--', color='red')\n",
    "plt.xlabel('Actual Danceability')\n",
    "plt.ylabel('Predicted Danceability')\n",
    "plt.title('Random Forest: Actual vs. Predicted Danceability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "\n",
    "feature_importance = pd.DataFrame({'Feature': features_cols, 'Importance': best_rf_model.feature_importances_})\n",
    "print(feature_importance.sort_values(by='Importance', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting Check I - Model Fit\n",
    "rf_model = RandomForestRegressor(n_estimators=250, max_depth=18, min_samples_split=15, min_samples_leaf=6, random_state=42)\n",
    "rf_model.fit(features_train, target_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting Check II - Prediction & Evaluation\n",
    "target_pred_rf = rf_model.predict(features_test)\n",
    "\n",
    "rmse_rf = np.sqrt(mean_squared_error(target_test, target_pred_rf))\n",
    "r2_rf = r2_score(target_test, target_pred_rf)\n",
    "\n",
    "target_pred_train = rf_model.predict(features_train)\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(target_train, target_pred_train))\n",
    "r2_train = r2_score(target_train, target_pred_train)\n",
    "\n",
    "# Training vs. Test performance\n",
    "print('Training RMSE: {:.4f}'.format(rmse_train))\n",
    "print('Training R² Score: {:.4f}'.format(r2_train))\n",
    "print('Test RMSE: {:.4f}'.format(rmse_rf))\n",
    "print('Test R² Score: {:.4f}'.format(r2_rf))\n",
    "\n",
    "# Overfitting Check: Big Gap?\n",
    "if r2_train - r2_rf > 0.1:\n",
    "    print('The model may be overfitting! Consider tuning hyperparameters or using a different model.')\n",
    "else:\n",
    "    print('No major overfitting detected.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verdict\n",
    "Decent up to ~61% but prone to serious overfitting, even with min_samples adjustment. Use classification, instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
